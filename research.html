<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Desmond Elliott at the University of Copenhagen</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/1-col-portfolio.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body>

    <!-- Navigation -->
    <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="index.html">Desmond Elliott</a>
            </div>
            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav">
                    <li>
                        <a href="research.html">Research Interests</a>
                    </li>
                    <li>
                        <a href="publications.html">Papers</a>
                    </li>
                    <li>
                        <a href="resources.html">Resources</a>
                    </li>
                    <li>
                        <a href="teaching.html">Teaching</a>
                    </li>
                    <li>
                        <a href="service.html">Service</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Page Content -->
    <div class="container">

	  <div class='col-md-8'>
				<h3>Multimodal Translation</h3>
<p>
We have been working on two tasks that go beyond English-language image description:
Multimodal Translation is the task of translating the description of an image,
given a corpus of parallel text aligned with images; Crosslingual Description
is the task of generating descriptions, given a corpus of
independently collected source- and target-language descriptions aligned with
images.
</p>

<p>
We proposed the first models for crosslingual image description (<a href="https://staff.fnwi.uva.nl/d.elliott/GroundedTranslation/">Elliott et al. (2015)</a>), released the <a href="http://www.statmt.org/wmt16/multimodal-task.html">Multi30K corpus</a>
of bilingually described images (<a href="http://aclweb.org/anthology/W16-3210">Elliott et al. (2016)</a>), devised a doubly-attentive multimodal neural translation model (<a href="http://aclweb.org/anthology/W16-2359">Calixto et al. (2016)</a>), and organised the first and second shared
tasks in both of these problems (<a href="http://aclweb.org/anthology/W16-2346">Specia et al. (2016)</a>).<br/><br/>

More recently, we proposed a model that learns visually grounded representations as an auxiliary task for multimodal translation (<a href="https://arxiv.org/abs/1705.04350">Elliott and Kádár (2017)</a>); our approach (UvA-TiCC) performed well against the state of the art for English-German translation in the <a href="http://www.statmt.org/wmt17/pdf/WMT18.pdf">2017 Multimodal Translation shared task</a>.
</p>

				<h3>Visual Dependency Representations</h3>
<p>
Visual Dependency Representations capture the prominent spatial relationships between the objects in an image (<a href="http://hdl.handle.net/1842/10524">Elliott 2015</a>). This representation has proven useful for automatic image description using detected objects (<a href="http://aclweb.org/anthology/P15-1005">Elliott and de Vries, 2015</a>) or gold-standard objects (<a href="http://aclweb.org/anthology/D/D13/D13-1128.pdf">Elliott and Keller, 2013</a>), and query-by-example image retrieval (<a href="http://aclweb.org/anthology/C/C14/C14-1012.pdf">Elliott, Lavrenko, and Keller, 2014</a>). There is a <a href="http://homepages.inf.ed.ac.uk/s0128959/dataset/index.html">treebank of annotated images</a> and an <a href="https://github.com/elliottd/vdrparser">off-the-shelf parsing model</a> for inducing the representations.
</p>
		</div>
        </div>

    </div>
    <!-- /.container -->
    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

</body>

</html>
