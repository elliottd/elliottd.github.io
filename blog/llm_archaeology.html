<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>LLM Archaeology by Desmond Elliott</title>

    <!-- Bootstrap Core CSS -->
    <link href="../css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="../css/1-col-portfolio.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <style type="text/css">
      ul {
         list-style: none;
         padding-left: 0px;
      }
      span.mono {
	 font-family: monospace;
      }
    </style>

</head>

<body>

    <!-- Navigation -->
    <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="index.html">Desmond Elliott</a>
            </div>
            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav">
                    <li>
                        <a href="about.html">About</a>
                    </li>
                    <li>
                        <a href="https://lampgroup.github.io/">Group</a>
                    </li>
                    <li>
                        <a href="publications.html">Papers</a>
                    </li>                    
		    <li>
			<a href="talks.html">Talks</a>
		    </li>
                    <li>
                        <a href="teaching.html">Teaching</a>
                    </li>
                    <li>
                        <a href="service.html">Service</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Page Content -->
    <div class="container">

    	<!-- Page Heading -->
        <div class="col-md-8">
		<h1>LLM Archaeology</h1>
		<h5>What we learn from things hiding in plain sight</h5>
		<a href="mailto:de@di.ku.dk">Desmond Elliott</a>, November 25, 2025
		<hr>
        	<h3>Introduction</h3>

		<img src="img/intro_images.png" style="width:100%;">
		<p>Figure 1: Examples of archaeological discoveries found (a) through professional survey at <a href="https://commons.wikimedia.org/w/index.php?curid=17377542">Göbekli Tepe</a>, (b) by luck for <a href="https://commons.wikimedia.org/w/index.php?curid=48964317">Terracotta Army</a>, and (c) by coincidence that sponge divers found the <a href="https://commons.wikimedia.org/w/index.php?curid=164647291">Antikythera mechanism</a> in a shipwreck.</p>

		<p>Archaeology reveals some of the secrets of human history through the traces left behind by our ancestors. Some of the most important discoveries are the result of pure luck. The Terracotta Army of Xi’an, China, is one such example. This grand funeral act for the First Emperor of China was described in ancient Chinese texts but lost to time; its location was only re-discovered in 1974 by a group of rural farmers. Other findings are the result of careful survey, sometimes using sophisticated tools to uncover what cannot be seen by the naked eye. One of the earliest examples of human village habitation is found at at Göbekli Tepe, Turkey, first discovered in 1963, but only excavated in 1995. It was, perhaps, predictable that this village could be discovered here, based on related Pre-Pottery Neolithic sites in the same region. Nevertheless, the insights about early farming culture changed our understanding of early human history. A little closer to the heart of computer scientists, the Antikythera mechanism was found in a shipwreck off the coast of Antikythera, Greece, in 1901, where it had been preserved for nearly 2,000 years. This mechanism is an analogue computer, constructed before 60 BCE, that was used to predict the position of celestial bodies. Such is the sophistication of this device, that there is no evidence of similar complexity until 1,400 years later <a href="https://www.nature.com/articles/444534a">(Marchant, 2006)</a>.</p>

<p>In contrast to studying historical physical artefacts, Large Language Models (LLM) should be the least likely place to search for hidden knowledge: the basic mathematics of these models is well understood. The secretive nature of for-profit frontier model development has given rise to <b>LLM Archaeology</b>. The tools of LLM Archaeologists are a solid grasp of linear algebra and probability theory, jailbreaking, and pdb, compared to the metal detectors, pickaxes, and radar surveys of traditional archaeology. LLM Archaeologists have reported findings through happenstance, by observing unexpected behaviour in publicly accessible models, and through careful study, by exploiting the mathematical properties of the underlying model. In this blog, we will cover five findings unearthed by LLM Archaeologists, and discuss the current status of those findings. We welcome comments on other discoveries that we have overlooked.</p>

		<hr>

		<h3>Inferring Training Data</h3>

		<img src="img/training_data.png" style="width: 100%; padding-top:16px;">

		<p>Figure 2: (a) Hayase et al. show how to infer the distribution of data used to train a tokenizer based on how BPE constructs merge lists (image source: <a href="https://openreview.net/forum?id=EHXyeImux0">Figure 1</a>). (b) Nasr et al. demonstrate a simple attack to extract training data from language models by forcing them to repeat the same token (image source: <a href="https://openreview.net/forum?id=vjel3nWP2a">Figure 1</a>).</p>

		<p>Training data is fuel that powers Large Language Models. Open-science initiatives, such as Pythia <a href="https://proceedings.mlr.press/v202/biderman23a.html">(Biderman et al. ICML'23)</a>, OLMo <a href="https://aclanthology.org/2024.acl-long.841/">(Groenveld et al. ACL'24)</a>, and PleIAs <a href="https://www.sciencedirect.com/science/article/pii/S1877050925025839">(Langlais et al.'25)</a> document everything about their data, making it possible to conduct further studies, e.g. data attribution <a href="https://aclanthology.org/2025.acl-demo.18/">(Liu et al. ACL'25)</a>. This is in stark contrast to for-profit LLM developers, but researchers have managed to learn some of the secrets of these models.</p>

		<p><b>Tokenizer Data:</b> <a href="https://openreview.net/forum?id=EHXyeImux0">Hayase et al. (NeurIPS'24)</a> show how to infer properties of the data used to train a tokenizer, which underpins how LLMs process text. Their approach, illustrated in Figure 2(a), is based on insights about how Byte-Pair Encoding token merge lists are created, and the implications for data used to train the tokenizer. This insight allowed them to accurately infer known properties about publicly disclosed models, and to predict the properties of private models. 

		<p><b>Pretraining Data:</b> <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting">Carlini et al. (USENIX'21)</a> show how to extract verbatim sequences from the GPT-2 language model. Their work is primarily focused on <a href="https://nicholas.carlini.com/writing/2025/privacy-copyright-and-generative-models.html">understanding privacy attacks</a> on language models, in which a model may reveal personal identifying information. More directly related to detecting copyright violations, <a href="https://aclanthology.org/2023.emnlp-main.458/">Karamolegkou et al. (EMNLP'23)</a> prompted open and closed LLMs with prefixes of copyrighted material from books. Finally, <a href="https://openreview.net/forum?id=vjel3nWP2a">Nasr et al. (COLM'24)</a> demonstrate a simple attack on ChatGPT that involved forcing it to generate the same token repeatedly, as shown in Figure 2(b). This eventually causes the model to “diverge” from its post-training objective and revert to its base model behaviour, in which it generates memorized training data. It was possible to extract strings up-to 4,000 characters long using this method.</p>

		<p><b>Status:</b> it has never been confirmed if Hayase et al. inferred the true data distribution of the tokenizers for GPT-4 and Claude. It has never been confirmed if Carlini et al, Nasr et al, or Karamolegkou et al. succeeded in extracting the training data from the GPT or Claude models.</p>

		<hr>

		<h3>Discovering Model Training Procedures</h3>

		<img src="img/model_training.png" style="width:85%; padding-top:16px;">
		<p>Figure 3: Parishad et al. stumbled upon evidence of how the Mistral-7B LLM was pretrained. This visualizations show the cosine similarity between encodings using causal attention and bidirection attention, across layers and token positions. The LLaMA2-7B model (a) shows low cosine similarities, especially at deeper layers, whereas Mistral-7B (b), shows the opposite, raising questions about how the model was pretrained (image source: <a href="https://openreview.net/forum?id=IW1PR7vEBf">Figure 5</a>.)</p>

		<p>Exactly how LLMs are trained is becoming increasingly shrouded in mystery but some noteworthy explanations remain in the open-weight and open-science literature. Nevertheless, given an LLM with unknown training process, researchers can sometimes discover behaviour that betrays tell-tale signatures of how a model was trained.</p>

		<p>In trying to convert a LLM into a sentence embedding model, <a href="https://openreview.net/forum?id=IW1PR7vEBf">BenhamGhader et al. (COLM'24)</a> stumbled upon evidence that the Mistral-7B LLM may have been pretrained with bidirectional attention. Their approach was to enable bidirectional attention in the LLM, which would allow the model to create better sentence-level representations than unidirectional attention. This new ability was trained using a combination of masked next token prediction and unsupervised contrastive learning. However, it was discovered that the Mistral-7B model constructed nearly-identical representations (Figure 3b) when using bidirectional attention or causal attention, compared to LLaMA2-7B (Figure 3a). This <i>innate ability</i> in Mistral-7B seemed to rule out the possibility that it was only trained on next-token prediction.</p>

		<p>In personal correspondence with Parishad BenhamGhader, she told me that the bidirectional attention finding came about as a result of a reviewer requesting additional models beyond LLaMa2-7B in the original manuscript. The behaviour that was observed for the LLaMA-7B model did not appear in the Mistral-7B model, which sparked the additional analysis of the cosine similarities at different token positions before and and after enabling bidirectional attention. It was here that it became clear that the representations from the Mistral-7B model were extremely similar with or without bidirectional attention. Further correspondence with Marius Mosbach, one of the collaborators on the project, revealed that he studied the Mistral inference code and found a flag in the <span class="mono">forward()</span> that could enable bidirectional attention. This led to speculation that the model was trained using a <a href="https://www.jmlr.org/papers/v21/20-074.html">PrefixLM-style objective</a>, even though there are no details on this topic in the six-page <a href="http://arxiv.org/abs/2310.06825">preprint</a>.</p>

		<p><b>Status:</b> it has never been confirmed whether this speculation is correct. At the time of writing, the original Mistral inference code is no longer publicly available.</p>

		<hr>

		<h3>Stealing Part of a Deployed Model</h3>

		<img src="img/stealing.png" style="width:100%; padding-top: 16px;">
		<p>Figure 3: (a) Carlini et al. show that the difference between consecutive singular values in an SVD decomposition of a matrix of final token logits is aligned with the embedding size of the PYthon 1.4B LLM, and that this size can be completed in 2,000 queries. (image sources: <a href="https://proceedings.mlr.press/v235/carlini24a.html">Figure 1 and 2</a>.)</p>

	<p><b>Hidden Dimension Size:</b> <a href="https://openreview.net/forum?id=oRcYFm8vyB">Finlayson et al. (COLM'24)</a> and <a href="https://proceedings.mlr.press/v235/carlini24a.html">Carlini et al. (ICML'24)</a> show how to determine the hidden size of an LLM. Carlini et al. rely on access to a model that returns the logits of the next token. Their method works by initializing an <span class="mono">n x l</span> matrix <span class="mono">Q</span>, where <span class="mono">n</span> is much larger than the hypothesized dimensionality of the target model, and <span class="mono">l</span> is the dimension of the returned logit vector. <span class="mono">Q</span> is populated with the logit vectors returned for each next token over a large set of random set of prefixes, based on the assumption that the <span class="mono">l</span>-dimension logits lie on the true <span class="mono">h</span>-dimension subspace of the model. The largest difference in consecutive pairs of singular values aligns with the hidden dimension of the model, as shown in Figure 3(a). Finlayson et al. used a similar technique to create an <i>LLM image</i>, from it was possible to estimate the embedding size of the API-based gpt-3.5-turbo model.</p>

	<p><b>Full Output Layer:</b> One can go further than inferring the hidden dimension side. Carlini et al. also show how to extract the entire output layer of language models by observing that in the SVD decomposition of <span class="mono">Q</span>, that <span class="mono">U</span> is a linear transformation of the output layer. They show that this can accurately extract the output embedding layer of open-weights models and use the same attack to steal the output layer of OpenAI deployed models.</p>

	<p><b>Status:</b> it was acknowledged that the method of Carlini et al. correctly extract the size of the models. It was also confirmed that they could steal the output layer of the ada and babbage models with a root-mean squared error of <span class="mono">5 x 10^-4</span> and <span class="mono">7 x 10^-4</span>, respectively, for just $4–12 dollars of API query credits. Finlayson et al. reported that several frontier models changed their APIs to prevent this information being stolen from their models.</p>

	<hr>

	<h3>Exfiltrating System Prompts</h3>

	<img src="img/exfiltration.png" style="width: 100%; padding-top: 16px;">
	<p>Figure 4: Zhang et al. show that simply asking LLMs to translate inputs into a different language (a) can reveal the model system prompt (b). (image source: <a href="https://openreview.net/forum?id=0o95CVdNuz">Figure 1 and Table 2</a>)</p>

	<p>This final finding is credited to <a href="https://openreview.net/forum?id=0o95CVdNuz">Zhang et al. (COLM'24)</a>, who show how to extract the system prompt of deployed LLMs using a jailbreak attack. The attack is relatively simple: a model is prompted to translate everything into a different language, e.g. German, Korean, Portuguese, or Spanish, as shown in Figure 4(a), which causes the model to generate its system prompt, as shown in Figure 4(b).</p>

	<p><b>Status:</b> The success of this technique can be <a href="https://docs.claude.com/en/release-notes/system-prompts">directly confirmed</a> for the Claude models.</p>

	<hr>

	<h3>Outlook</h3>

	<p>The knowledge unearthed in these studies has not been lost to time: these models are not sitting at the bottom of the Mediterranean Sea, nor are they resting underneath the fields of Xi’an. Instead, researchers are discovering knowledge that is being secreted away inside startups and corporations. This behaviour is often justified by the claim that organizations need to maintain their competitive edge, even though <a href="https://www.theverge.com/2023/7/10/23790132/google-memo-moat-ai-leak-demis-hassabis">they have no moat</a>.</p>

	<p>What is driving the people that make these discoveries? Clearly, some stumble upon their knowledge by complete chance. Perhaps some researchers enjoy the feeling of deeply understanding a complex system. It could also be that that this is a viable line of research for the GPU-poor. Whatever the motivation, so much of what is being rediscovered is built upon the shoulders of publicly funded research. We could, collectively, reach much further.</p>

	<hr>

	<h3>Acknowledgements</h3>

	<p>This blog was inspired by discussions in Vices and Versa during a research visit funded by the <a href="https://ivado.ca/en/thematic-programs/autonomous-llm-agents-risks-and-scientific-challenges/">IVADO Thematic Semester on Autonomous Agents</a>. Thanks to Marius Mosbach and Stella Frank for providing critical feedback on an earlier draft of this blog.</p>

	</div>
    </div>
    <!-- /.container -->
    <!-- jQuery -->
    <script src="../js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="../js/bootstrap.min.js"></script>

</body>

</html>
